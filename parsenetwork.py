import os

PATH_TO_TEXT_FILES = '/Users/ejw675/Downloads/llm-social-network/text-files'  # folder holding text files, typically GPT output

# place personas in a hash table: key = name, value = features
# inefficient; generatenetwork contains the same function
def parse_personas_from_gpt_output(node_fn):
    node_path = os.path.join(PATH_TO_TEXT_FILES, node_fn)
    assert os.path.isfile(node_path)
    
    personas = {} # hash table
    
    with open(node_path, 'r') as f:
        # move cursor to start of list
        lines = f.readlines()
        input = lines[4]
        input = input[16:len(input)-2]
        
        # add each "node" (person) to hash table
        nodes = input.split('\\n')
        for node in nodes:
            node = node.replace('.', ',').replace(' -', ',')
            features = node.split(', ')
            personas[features[1]] = features
            # name : [index, name, gender, age, ethnicity, religion, politics]
            
    print('# of distinct personas: ', len(personas)) #debugging
    return personas
    
# get homophily measures from network generated by gpt
def parse_metrics_from_gpt_network(edge_fn, personas):
    edge_path = os.path.join(PATH_TO_TEXT_FILES, edge_fn)
    assert os.path.isfile(edge_path)
    cr = [0] * 5 # cross-relationships: gender, age, ethnicity, religion, politics
    
    with open(edge_path, 'r') as f:
        # move cursor to start of list
        lines = f.readlines()
        input = lines[4]
        input = input[16:len(input)-2]
        
        pairs = input.split('\\n') # distinguish pairs
        pairs.pop(len(pairs) - 1) # in case limited by length
        for pair in pairs:
            p1, p2 = pair.split(', ')
            p1 = p1.strip('- ')
            f1 = personas[p1]
            f2 = personas[p2]
            for i in range(5): # calculate differences in features for this pair
                if (i == 1):
                    cr[1] += abs(int(f1[3]) - int(f2[3]))
                else:
                    if (f1[i + 2] != f2[i + 2]):
                        cr[i] += 1
    
    for i in range(5): # average to get proportion of hetereogenous connections
        cr[i] = cr[i] / len(pairs)
    return cr
    
def parse_metrics_from_gpt_personas(personas):
    cr = [0] * 5 # cross-relationships: gender, age, ethnicity, religion, politics
    
    # iterate through all possible friendship pairs
    for p1 in personas:
        for p2 in personas:
            f1 = personas[p1]
            f2 = personas[p2]
            for i in range(5): # calculate differences in features for this pair
                if (i == 1):
                    cr[1] += abs(int(f1[3]) - int(f2[3]))
                else:
                    if (f1[i + 2] != f2[i + 2]):
                        cr[i] += 1
    
    for i in range(5): # average to get proportion of hetereogenous connections
        cr[i] /= len(personas) * (len(personas) - 1)
    return cr

if __name__ == '__main__':
    # could change this to read from command line
    personas = parse_personas_from_gpt_output('personas.txt')
    expected_cr = parse_metrics_from_gpt_personas(personas)
    print('Expected proportion of cross-relationships: ', expected_cr)
    
    for i in range(1): # change to number of generated networks to parse
        network = 'test' + str(i) + '.txt'
        actual_cr = parse_metrics_from_gpt_network(network, personas)
    
        homophily = [0] * 5 # comparison between actual/expected homophily
        for j in range(4):
            homophily[j] = actual_cr[j] / expected_cr[j]
        print('Network ', i, ' actual proportion: ', homophily)
